# -----------------------------------------------------------

base_learning_rate = 0.001

# from_logits=False says to NOT calculate sigmoid/softmax, because it is already used in the last Dense layer 
optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate) # OR optimizer='adam'
loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)  # OR loss='sparse_categorical_crossentropy'
metrics = ['accuracy'] # OR metrics=['acc']
model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# Make up a directory where the checkpoints will be saved
checkpoint_dir = './training_checkpoints'

# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

# Saves the model weights every 2 epoches, weights are saved in .h5 format
# Calculate the number of batches per epoch
n_batches = len(X_train_selected) / BATCH_SIZE
n_batches = math.ceil(n_batches)    # round up the number of batches
num_of_epoches2save = 2
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, 
                                                         save_weights_only=True,
                                                        save_freq=num_of_epoches2save*n_batches)

# Save the weights using the `checkpoint_path` format
model.save_weights(checkpoint_prefix.format(epoch=0))

EPOCHS = 20

# Train the model: see if the loss and accuracy can be better for this dataset
history = model.fit(train_ds_4d, validation_data=test_ds_4d,
                   epochs=EPOCHS, 
                    # steps_per_epoch=STEPS_PER_EPOCH,
                    callbacks=[checkpoint_callback, early_stopping]
                   )

# -----------------------------------------------------------

# List all the saved checkpoints
# https://www.tensorflow.org/tutorials/keras/save_and_load
os.listdir(checkpoint_dir)

# -----------------------------------------------------------

# List the latest checkpoint
latest = tf.train.latest_checkpoint(checkpoint_dir)
latest

# How to load a model from a checkpoint
# [Step 0] clear model from memory
# [Step 1] Define the model 
# model = MPCNN_arch(XXX)

# [Step 2] Load the weights
# model.load_weights('/kaggle/working/training_checkpoints/ckpt_16')

# [Step 3] Use the model as if one had just trained it

# -----------------------------------------------------------

# Save model using checkpoint
!zip training_checkpoints.zip training_checkpoints

# -----------------------------------------------------------



# -----------------------------------------------------------
### [0] Log-in to GCP account
# -----------------------------------------------------------

# Copy-paste key.json generated on GCP

# -----------------------------------------------------------

!gcloud auth login --quiet --cred-file=key.json --force --project textprocessing1

# -----------------------------------------------------------

!gcloud config set project textprocessing1 --quiet

# -----------------------------------------------------------

## Way 0: bash
!gcloud storage cp --recursive /kaggle/working/training_checkpoints.zip gs://tensorflowjsmodels0/

# -----------------------------------------------------------

## Way 1: python
# Send the results to a Cloud Bucket
# pip install google-cloud-storage

# Setup appropriate credentials to access your Google Cloud project. Use either `GOOGLE_APPLICATION_CREDENTIALS` environment 
# variable to the path of your service account key file or provide the path directly in your code.

from google.cloud import storage

# Create a client instance
storage_client = storage.Client(project='textprocessing1')

bucket_name = "tensorflowjsmodels0"
bucket = client.bucket(bucket_name)

# Way 0: upload from file path
file_path = '/kaggle/working/training_checkpoints.zip'
blob_name = 'blob-name'
blob = bucket.blob(blob_name)

# Upload the file to the bucket
blob.upload_from_filename(file_path)

# Way 1; upload string data 
# data = 'your-data'
# blob = bucket.blob(blob_name)

# Upload the data to the bucket
# blob.upload_from_string(data)

# -----------------------------------------------------------


# -----------------------------------------------------------

# -----------------------------------------------------------


# -----------------------------------------------------------


# -----------------------------------------------------------

# -----------------------------------------------------------
